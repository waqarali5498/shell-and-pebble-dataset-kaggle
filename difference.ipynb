{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, there is a difference between fine-tuning and transfer learning, although they are related concepts often used together in deep learning. Let's break down both terms:\n",
    "\n",
    "### Transfer Learning\n",
    "Transfer learning involves taking a pre-trained model (trained on a large dataset like ImageNet) and using it as a starting point to solve a different but related task. This can significantly reduce the amount of data and time required to train a model from scratch.\n",
    "\n",
    "There are two main approaches to transfer learning:\n",
    "\n",
    "1. **Feature Extraction**: Here, you use the pre-trained model as a fixed feature extractor. You remove the top (fully connected) layers of the pre-trained model and add your own custom layers for the new task. The pre-trained layers are not updated during training; only the new layers are trained.\n",
    "   - **Example**: Using VGG16's convolutional layers to extract features from images and then adding a new fully connected layer to classify these features for a different task.\n",
    "\n",
    "2. **Fine-Tuning**: This involves unfreezing some of the top layers of the pre-trained model and jointly training both the newly added layers and the pre-trained layers. This allows the model to adapt pre-trained features to the new task, which can lead to better performance.\n",
    "   - **Example**: Unfreezing the last few convolutional layers of VGG16 and training them along with new fully connected layers added for the specific task.\n",
    "\n",
    "### Fine-Tuning\n",
    "Fine-tuning is a specific type of transfer learning where you adjust (or \"fine-tune\") the pre-trained model's weights on your new task. It generally involves unfreezing a few layers from the pre-trained model and retraining them along with the new layers you have added.\n",
    "\n",
    "**Steps for Fine-Tuning:**\n",
    "1. Load a pre-trained model without the top layer.\n",
    "2. Add custom top layers for your specific task.\n",
    "3. Freeze the layers of the pre-trained model to preserve the learned features.\n",
    "4. Train the new top layers on your dataset.\n",
    "5. Unfreeze some of the top layFine-tuning is a specific type of transfer learning where you adjust (or \"fine-tune\") the pre-trained model's weights on your new task. It generally involves unfreezing a few layers from the pre-trained model and retraining them along with the new layers you have added.​\n",
    "\n",
    "​\n",
    "​\n",
    "\n",
    "**Steps for Fine-Tuning:**​\n",
    "\n",
    "1. Load a pre-trained model without the top layer.​\n",
    "\n",
    "2. Add custom top layers for your specific task.​\n",
    "\n",
    "3. Freeze the layers of the pre-trained model to preserve the learned features.​\n",
    "\n",
    "4. Train the new top layers on your dataset.​\n",
    "\n",
    "5. Unfreeze some of the top layers of the pre-trained model.​\n",
    "\n",
    "6. Continue training on your dataset with a lower learning rate to fine-tune the model.​\n",
    "\n",
    "​ers of the pre-trained model.\n",
    "6. Continue training on your dataset with a lower learning rate to fine-tune the model.\n",
    "\n",
    "### Example: Transfer Learning and Fine-Tuning with VGG16\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Load the VGG16 model without the top layer\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Add custom layers on top of the pre-trained model\n",
    "model = Sequential([\n",
    "    base_model,\n",
    "    GlobalAveragePooling2D(),\n",
    "    Dense(1024, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(10, activation='softmax')  # Assuming you have 10 classes\n",
    "])\n",
    "\n",
    "# Freeze the layers of the base model (for transfer learning)\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Prepare your data\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    'data/train',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    'data/train',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "# Train the new layers\n",
    "model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.samples // validation_generator.batch_size,\n",
    "    epochs=10\n",
    ")\n",
    "\n",
    "# Unfreeze some layers of the base model (for fine-tuning)\n",
    "for layer in base_model.layers[-4:]:  # Unfreeze the last 4 layers\n",
    "    layer.trainable = True\n",
    "\n",
    "# Recompile the model with a lower learning rate\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fine-tune the model\n",
    "model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.samples // validation_generator.batch_size,\n",
    "    epochs=10\n",
    ")\n",
    "```\n",
    "\n",
    "In this example:\n",
    "- **Transfer Learning**: We initially freeze the pre-trained layers and train the new layers.\n",
    "- **Fine-Tuning**: We then unfreeze some of the pre-trained layers and continue training with a lower learning rate to adapt the model to the new task."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
